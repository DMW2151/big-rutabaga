---
layout: default
title:  DataKind DataDive - March 2021
date:   2021-03-17 21:46:26 -0500
permalink: /posts/datadive
---

A few weeks ago I attended a [DataDive](https://www.datakind.org/blog/powering-public-data-for-communities-datakind-hosts-virtual-datadiver-event) sponsored by DataKind. A DataDive is not dissimilar from a hack-a-thon, but focuses on solving specific data questions or building specific data products for a few partner organization instead of splintering into many small, competing teams. The project I found myself working on was focused on executing analysis and creating maps for the [New America Foundation](https://www.newamerica.org/future-land-housing/). Immediately I was excited to implement some sort of geo-spatial data pipeline that could serve dynamically generated maps. Furthermore, deploying a [tile server](https://wiki.openstreetmap.org/wiki/Tile_servers) has been something that I've been meaning to do for at least 6 months now.

I was disappointed to discover that the organization wanted a series of research questions answered rather than a data platform or a piece of software developed over the course of the event. Although I volunteered as a "Data Ambassador", which meant that the majority of my time for the weekend would be spent on pair programming with other volunteers, maintaining infrastructure, answering data questions, and teaching the basics of git, Docker, or PostgreSQL, it was still quite a blow to see a list of dozens of questions in lieu of a product spec.

However, the simplicity of the project gave me other ideas. Ideas that perhaps could facilitate other volunteers' workflows while still allowing me to write some code (at least in the days leading up to the event). In the past, I noticed quite a few volunteers struggled with setting up environments for running other volunteer's code/analysis. With that in mind, I came in with the (perhaps too ambitious) plan of deploying a Jupyterhub instance for my volunteers to use throughout the event.

## Architecture

[Jupyterhub](https://jupyterhub.readthedocs.io/en/stable/) is a multi-user server which can spawn and terminate instances of Jupyter notebooks. As a rule, I dislike the idea of notebooks. This is [not](https://godatadriven.com/blog/write-less-terrible-code-with-jupyter-notebook/) a [novel](http://web.eecs.utk.edu/~azh/blog/notebookpainpoints.html) [idea](https://towardsdatascience.com/the-case-against-the-jupyter-notebook-d4da17e97243), but they're absolutely suited for events such as this. I was hoping to be able to spawn a proxy for each volunteer using the following tools:

![Hub](/big-rutabaga/diagrams/HubDiagram.png)

- JupyterHub to spawn, manage, and proxies multiple instances of the Jupyter notebook server running on EC2. ✅

- PostgreSQL/PostGIS running on the same EC2 instance as a database to hold the starting data for the event. ✅

- A Docker container with common geo-spatial, mapping, data science, and analysis libraries pre-loaded into the notebook environment. ✅

- Github Actions to sync the `main` branch onto the EC2 instance, so volunteers would always have access to each other's work. ❌

- Nginx as a reverse proxy and to enable HTTPs ✅

## PostGIS Deployment

Bought a t2.large from AWS, installed PostgreSQL and PostGIS and then modified permissions to allow users to connect to the DB. Done!

I (naively) set the listen address to `0.0.0.0` for users to connect from any IP around the world. Not great, in fact, very bad. I immediately got hammered with hundreds of login attempts for the root `postgres` account.

Luckily, I had HBA rules that disallowed `postgres` from connecting from anywhere but `localhost` and was able to remedy the situation by accepting connections only on `localhost` and the IP of the JupyterHub server. This way all volunteers could use the DB, so long as they were also willing to get on-boarded to the hub.

## JupyterHub Deployment

JupyterHub also happens to be remarkably easy to deploy. I took a heavier instance this time ([note on Hub sizing](https://tljh.jupyter.org/en/latest/howto/admin/resource-estimation.html)) and wrote up an Ansible playbook to:

- Install `Conda` , `JupyterHub`, and all dependencies

- Set up and start the service using `systemd`

- Copy certs and start Nginx (in retrospect, purchasing a domain for this was waste of $12, but it's a great name that I'm sure I'll resurface for something down the line)

With JupyterHub there are several options for authentication and server spawning, these were the most difficult choices I made during this process.

### Auth

In deploying JupyterHub (as of v1.3) there are a [few ways](https://jupyterhub.readthedocs.io/en/stable/reference/authenticators.html) to authenticate users to a Hub. None of them were particuarly satisfying for my use case.

PAM authentication allows for authentication given there is a system user on the Hub host. Thinking I'd have 30-50 users, I immediately nixed this idea because I didn't want volunteers spending the weekend waiting on me to run `sudo useradd ...`

Dummy authentication allows for any user to authenticate so long as they have the correct Hub global password. This is what I should have opted for, but the bells and whistles of Github Oauth got me.

GitHub Oauth allows anyone to access the hub so long as they have a Github to authenticate with. In retrospect, Not great, essentially granting access to the whole world.

### Spawning

A Spawner starts each single-user notebook server. Users can spawn their single notebook in a container, or a shared (or common) location in the local filesystem. You may read more about spawners [here](https://jupyterhub.readthedocs.io/en/stable/reference/spawners.html#examples).

It seemed that I could have a high degree of trust in my users ([Note on Security](https://jupyterhub.readthedocs.io/en/stable/reference/websecurity.html)) and using the DockerSpawner allowed them the greatest freedom to manipulate their own environments without interfering with each others analysis.

When a user logged in, a docker container would spin up with a Python 3.8 environment containing a nice set of ML tools and `gdal`, `ogr2ogr`, etc.

### Github CI

Saved this until the night before the volunteers were set to arrive but ended up never implementing this. Ideally each commit to `./data` in the main branch would push changed datasets to this environment.

### Postscript

By the day prior, the writing was on the wall that this whole endeavor would mostly be pedagogical. Turnout was going to be low, about half of the volunteers wouldn't be familiar with Github, and the project had been tilted further towards run-of-the-mill `pandas` and `numpy` style analysis.

I got quite nervous that I was being selfish by pushing this platform I cobbled together in two days onto volunteers when their laptops would suffice for >99% of what they might want to do.

I did however, quite enjoy the experience of working through these problems, helping the few volunteers who were interested in getting on-boarded to the platform, and generally working with everyone over the weekend.

### Open Questions - Glaring flaws

- Github Oauthentication is a problem for many users, a `DummyAuthenticator` would have been a better choice for this use case.
